{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18934/4125185198.py:18: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  DATA.fillna(DATA.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ESN = pd.read_csv('../content/csv/FINALDATASET_EastSlopesNorth.csv')\n",
    "WSN = pd.read_csv('../content/csv/FINALDATASET_WestSlopesNorth.csv')\n",
    "WSS = pd.read_csv('../content/csv/FINALDATASET_WestSlopesSouth.csv')\n",
    "\n",
    "assert all(ESN.columns) == all(WSN.columns) and all(ESN.columns) == all(WSS.columns), \\\n",
    "    'All dataframes should have the same columns'\n",
    "\n",
    "# Combine dataframes\n",
    "\n",
    "DATA = pd.concat([ESN, WSN, WSS], axis=0, ignore_index=True)\n",
    "\n",
    "# Show any rows with NaN\n",
    "DATA[DATA.isna().any(axis=1)]\n",
    "\n",
    "# Todo: try dropping NaN columns instead\n",
    "DATA.fillna(DATA.mean(), inplace=True)\n",
    "\n",
    "# Todo: should we try to use these?\n",
    "DATA.drop(['Date', 'Area'], axis=1, inplace=True)\n",
    "\n",
    "# Can uncomment to make prediction better\n",
    "DATA.drop(['Yesterday Danger Above Treeline', 'Yesterday Danger At Treeline', 'Yesterday Danger Below Treeline'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = DATA.drop('At Treeline', axis=1), DATA['At Treeline']\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "for train_index,val_index in kf.split(X):\n",
    "    X_train,X_val = X.iloc[train_index],X.iloc[val_index],\n",
    "    y_train,y_val = y.iloc[train_index],y.iloc[val_index],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': 42,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_booster = GradientBoostingClassifier(learning_rate=0.1, random_state=42)\n",
    "gradient_booster.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.45      0.48        11\n",
      "           2       0.88      0.82      0.85        55\n",
      "           3       0.78      0.95      0.86        19\n",
      "           4       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.81        88\n",
      "   macro avg       0.73      0.81      0.76        88\n",
      "weighted avg       0.81      0.81      0.80        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradient_booster.fit(X_train,y_train)\n",
    "print(classification_report(y_val,gradient_booster.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate for Validation: 0.1\n",
      "Best N Estimators for Validation: 34\n",
      "Best Max Features for Validation: 13\n",
      "Best Max Depth for Validation: 2\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "best_learning_rate, best_score = 0.1, 0\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    validation = gb_clf.score(X_val, y_val)\n",
    "    if validation > best_score:\n",
    "        best_learning_rate = learning_rate\n",
    "        best_score = validation\n",
    "\n",
    "    # print(\"Learning rate: \", learning_rate)  \n",
    "    # print(\"\\tAccuracy score (training): \\t{0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    # print(\"\\tAccuracy score (validation): \\t{0:.3f}\".format(gb_clf.score(X_val, y_val)))\n",
    "print('Best Learning Rate for Validation:', best_learning_rate)\n",
    "\n",
    "n_est_list = [i for i in range(1, 100)]\n",
    "\n",
    "best_n_estimator, best_score = 20, 0\n",
    "\n",
    "for n_est in n_est_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=n_est, learning_rate=0.1, max_features=2, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    validation = gb_clf.score(X_val, y_val)\n",
    "    if validation > best_score:\n",
    "        best_n_estimator = n_est\n",
    "        best_score = validation\n",
    "\n",
    "    # print(\"Learning rate: \", learning_rate)  \n",
    "    # print(\"\\tAccuracy score (training): \\t{0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    # print(\"\\tAccuracy score (validation): \\t{0:.3f}\".format(gb_clf.score(X_val, y_val)))\n",
    "print('Best N Estimators for Validation:', best_n_estimator)\n",
    "\n",
    "features_list = [i for i in range(1, 20)]\n",
    "\n",
    "best_features_estimator, best_score = 2, 0\n",
    "\n",
    "for features in features_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=34, learning_rate=0.1, max_features=features, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    validation = gb_clf.score(X_val, y_val)\n",
    "    if validation > best_score:\n",
    "        best_features_estimator = features\n",
    "        best_score = validation\n",
    "\n",
    "    # print(\"Learning rate: \", learning_rate)  \n",
    "    # print(\"\\tAccuracy score (training): \\t{0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    # print(\"\\tAccuracy score (validation): \\t{0:.3f}\".format(gb_clf.score(X_val, y_val)))\n",
    "print('Best Max Features for Validation:', best_features_estimator)\n",
    "\n",
    "depth_list = [i for i in range(1, 20)]\n",
    "\n",
    "best_depth_estimator, best_score = 2, 0\n",
    "\n",
    "for depth in depth_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=34, learning_rate=0.1, max_features=13, max_depth=depth, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    validation = gb_clf.score(X_val, y_val)\n",
    "    if validation > best_score:\n",
    "        best_depth_estimator = depth\n",
    "        best_score = validation\n",
    "\n",
    "    # print(\"Learning rate: \", learning_rate)  \n",
    "    # print(\"\\tAccuracy score (training): \\t{0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    # print(\"\\tAccuracy score (validation): \\t{0:.3f}\".format(gb_clf.score(X_val, y_val)))\n",
    "print('Best Max Depth for Validation:', best_depth_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy score (training): \t0.890\n",
      "\tAccuracy score (validation): \t0.852\n"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(n_estimators=34, learning_rate=0.1, max_features=13, max_depth=2, random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\tAccuracy score (training): \\t{0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "print(\"\\tAccuracy score (validation): \\t{0:.3f}\".format(gb_clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted less dangerous than actual: 0 0.00%\n",
      "Predicted more dangerous than actual: 52 11.79%\n",
      "WAY off: 0 0.00%\n"
     ]
    }
   ],
   "source": [
    "predicted = gb_clf.predict(DATA.drop('At Treeline', axis=1))\n",
    "actual = DATA['At Treeline']\n",
    "assert len(predicted) == len(actual)\n",
    "over_predict = 0\n",
    "under_predict = 0\n",
    "way_off = 0\n",
    "for i in range(len(actual)):\n",
    "    p = predicted[i]\n",
    "    a = actual[i]\n",
    "    diff = a - p\n",
    "    if diff == 1:\n",
    "        under_predict += 1\n",
    "    if diff == -1:\n",
    "        over_predict += 1\n",
    "    if diff < -1 or diff > 1:\n",
    "        way_off += 1\n",
    "        print('WAY off: predicted:', p, 'actual:', a)\n",
    "under_pct = '{0:.2f}'.format(under_predict * 100.0 / len(actual))\n",
    "over_pct = '{0:.2f}'.format(over_predict * 100.0 / len(actual))\n",
    "way_pct = '{0:.2f}'.format(way_off * 100.0 / len(actual))\n",
    "print('Predicted less dangerous than actual:', under_predict, f'{under_pct}%')\n",
    "print('Predicted more dangerous than actual:', over_predict, f'{over_pct}%')\n",
    "print('WAY off:', way_off, f'{way_pct}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
